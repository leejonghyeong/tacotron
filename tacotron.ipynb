{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "# session_id, text1_ids, text2_ids, label(default: -1)\n",
    "ExampleType = Tuple[str, List[int], List[int], float]\n",
    "\n",
    "def load_examples(dataset_path: str) -> List[ExampleType]:\n",
    "    examples = []\n",
    "    with open(dataset_path) as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        for example in csv_reader:\n",
    "            text1_ids = [int(token_id) + 1 for token_id in example[\"sentence1\"].split(\" \")]\n",
    "            text2_ids = [int(token_id) + 1 for token_id in example[\"sentence2\"].split(\" \")]\n",
    "            label = float(example[\"label\"]) if \"label\" in example else -1.0\n",
    "            examples.append((example[\"id\"], text1_ids, text2_ids, label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_examples = load_examples(\"train.csv\")\n",
    "dev_split_index = int(len(trainable_examples) * 0.8)\n",
    "\n",
    "train_examples = trainable_examples[:dev_split_index]\n",
    "dev_examples = trainable_examples[dev_split_index:]\n",
    "test_examples = load_examples(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as fnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim import Adadelta\n",
    "#from transformers import get_constant_schedule_with_warmup\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "embedding_size = 256\n",
    "enc_bank_size = 16\n",
    "enc_proj_size = 128\n",
    "ppn_bank_size = 8\n",
    "ppn_proj_size = 80\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size,  2*hidden_size)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, hidden_size//2)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = F.relu(self.fc1(inputs))\n",
    "        outputs = self.dropout1(outputs)\n",
    "        outputs = F.relu(self.fc2(outputs))\n",
    "        outputs = self.dropout2(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBHG(nn.Module):\n",
    "    def __init__(self, hidden_size, K, proj_out):\n",
    "        '''\n",
    "        Args:\n",
    "            K(obj: int)\n",
    "                bank size\n",
    "            proj_out\n",
    "                dimension of output of conv1d projection\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConstantPad1d(((k-1)//2, (k-1)//2 +1),0),\n",
    "                nn.Conv1d(128, 128, k)\n",
    "            ) \n",
    "            for k in range(K)]) \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "        self.conv1d_proj = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3),\n",
    "            nn.Conv1d(256, proj_out, 3)\n",
    "        )\n",
    "        self.highway = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128, 128),\n",
    "                F.relu()\n",
    "            )\n",
    "            for _ in range(4)])\n",
    "\n",
    "        self.gru = nn.GRU(128, 128, num_layers = 1, batch_first = True, bidirectional = True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #inputs shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        #Conv1D bank and stacking\n",
    "        conv_output = []\n",
    "        for conv in self.conv1d_list:\n",
    "            conv_output.append(conv(inputs))\n",
    "        bank_output = torch.stack(conv_output, dim = -1)\n",
    "        bank_output = self.batch_norm(bank_output)\n",
    "\n",
    "        #Maxpooling\n",
    "        maxpool_output = self.maxpool(bank_output)\n",
    "\n",
    "        #Conv1D projection and Residual connection\n",
    "        proj_output = self.conv1d_proj(maxpool_output) + maxpool_output\n",
    "        proj_output = self.batch_norm(proj_output)\n",
    "        \n",
    "        #Highway network\n",
    "        highway_output = proj_output\n",
    "        for layer in self.highway:\n",
    "            highway_output = layer(highway_output)\n",
    "\n",
    "        #bidirectional GRU\n",
    "        CBHG_output = self.gru(highway_output)\n",
    "\n",
    "        return CBHG_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhAttention():\n",
    "    def __init__(self, hidden_dim) -> None:\n",
    "        '''\n",
    "        Args:\n",
    "            hidden_dim(`int`):\n",
    "                hidden_size of encoder outputs\n",
    "        '''\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, key, query):\n",
    "        query_repeated = query.unsqueeze(1).repeat(1,key.size(1),1,1)\n",
    "\n",
    "        attention = self.v(self.tanh(self.W1(key) + self.W2(query_repeated)))\n",
    "        weight = nn.Softmax(dim=1)(attention)\n",
    "        context = torch.matmul(weight.transpose(1,2), key).squeeze(1)\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, embedding_size:int):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.pre_net = PreNet(embedding_size, hidden_size, dropout)\n",
    "        self.cbhg = CBHG(hidden_size, enc_bank_size, enc_proj_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        word_emb = self.char_emb(inputs)\n",
    "        prenet_outputs = self.pre_net(word_emb)\n",
    "        enc_outputs = self.cbhg(prenet_outputs)\n",
    "\n",
    "        return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.pre_net = PreNet()\n",
    "        self.attention = TanhAttention(2*hidden_size)\n",
    "        self.attn_rnn = nn.GRU(hidden_size, hidden_size, num_layers = 1)\n",
    "        self.dec_rnn1 = nn.GRU(4*hidden_size, 2*hidden_size, num_layers = 2)\n",
    "        self.dec_rnn2 = nn.GRU(2*hidden_size, 2*hidden_size, num_layers = 2)\n",
    "        self.fc = nn.Linear(hidden_size, 2 * hidden_size)\n",
    "    \n",
    "    def forward(self, enc_outputs, dec_inputs, reduction):\n",
    "        '''\n",
    "        Args:\n",
    "            dec_inputs (batch_size, 256, dec_steps)\n",
    "        '''\n",
    "        batch_size = dec_inputs.size(0)\n",
    "        total_steps = dec_inputs.size(2) // reduction\n",
    "        inputs = torch.cat([\n",
    "            torch.zero(batch_size, hidden_size),\n",
    "            dec_inputs[:,:,1:]\n",
    "        ], dim=-1)\n",
    "        \n",
    "\n",
    "        attn_hidden = torch.zero(batch_size, hidden_size)\n",
    "        dec_hidden1 = torch.zero(batch_size, hidden_size)\n",
    "        dec_hidden1 = torch.zero(batch_size, hidden_size)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(total_steps):\n",
    "            if self.training:\n",
    "                prenet_inputs = inputs[i]\n",
    "            else:\n",
    "                prenet_inputs = mel_outputs\n",
    "\n",
    "            prenet_outputs = self.pre_net(prenet_inputs)\n",
    "\n",
    "            attn_outputs, attn_hidden = self.attn_rnn(prenet_outputs, attn_hidden)\n",
    "            context = self.attention(enc_outputs, attn_outputs)\n",
    "\n",
    "            dec_inputs = torch.cat([attn_outputs, context], dim = -1)\n",
    "            dec_outputs, dec_hidden1 = self.dec_rnn1(dec_inputs, dec_hidden1) + dec_inputs\n",
    "\n",
    "            dec_outputs, dec_hidden2 = self.dec_rnn2(dec_outputs, dec_hidden2) + dec_outputs\n",
    "\n",
    "            dec_outputs = self.fc(dec_outputs).view(batch_size, total_steps, -1)\n",
    "            mel_outputs = dec_outputs[:,:,-1]\n",
    "\n",
    "            outputs.append(mel_outputs)\n",
    "        \n",
    "        return torch.stack(outputs, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import GriffinLim\n",
    "sample_rate = 24000\n",
    "frame_shift = 12.5\n",
    "frame_length = 50\n",
    "n_iter = 30\n",
    "n_fft = 2480\n",
    "hop_length = int(frame_shift * 0.001/sample_rate)\n",
    "win_length = int(frame_length * 0.001/sample_rate)\n",
    "\n",
    "class Vocoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cbhg = CBHG(hidden_size, ppn_bank_size, ppn_proj_size)\n",
    "        self.griffinlim = GriffinLim(n_fft, n_iter, win_length, hop_length, window_fn=torch.hann_window)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.cbhg(inputs)\n",
    "        outputs = self.griffinlim(outputs)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "830fc09f41f4a8127b783ad2119e0f8b44080ccbdb0580da15d6f44fb7345381"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

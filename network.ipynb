{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List, Dict, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as fnn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim import Adadelta\n",
    "#from transformers import get_constant_schedule_with_warmup\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_\n",
    "from tqdm.notebook import tqdm\n",
    "from .modules import PreNet, CBHG, TanhAttention, DecoderRNN\n",
    "from .hyperparameter import hyperparameters as hp\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, bank_size, proj_size):\n",
    "        super().__init__()\n",
    "        self.char_emb = nn.Embedding(hp.vocab_size, hp.embedding_size, padding_idx=0)\n",
    "        self.pre_net = PreNet(hp.embedding_size, hidden_size, hp.dropout)\n",
    "        self.cbhg = CBHG(hidden_size, bank_size, proj_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        word_emb = self.char_emb(inputs)\n",
    "        prenet_outputs = self.pre_net(word_emb)\n",
    "        enc_outputs = self.cbhg(prenet_outputs)\n",
    "\n",
    "        return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            hidden_size\n",
    "                - hidden_size of pre-net\n",
    "        '''\n",
    "        self.hidden_dim = hp.hidden_dim\n",
    "        self.device = hp.device\n",
    "        self.reduction = hp.reduction\n",
    "        self.dropout = hp.dropout\n",
    "\n",
    "        self.pre_net = PreNet(self.hidden_dim, hidden_size, self.dropout)\n",
    "        self.attention = TanhAttention(2*self.hidden_dim)\n",
    "        self.attn_rnn = nn.GRU(self.hidden_dim, self.hidden_dim, num_layers = 1)\n",
    "        self.dec_rnn = DecoderRNN(4*self.hidden_dim, 2*self.hidden_dim)\n",
    "        self.fc = nn.Linear(self.hidden_dim, 2 * self.hidden_dim)\n",
    "    \n",
    "    def forward(self, enc_outputs, dec_inputs):\n",
    "        '''\n",
    "        Args:\n",
    "            dec_inputs:\n",
    "                - a batch of Ground truth frames\n",
    "                - Tensor of shape (batch_size, n_frames, n_fft // 2 + 1)\n",
    "            reduction factor:\n",
    "                - default = 2\n",
    "        '''\n",
    "        batch_size = dec_inputs.size(0)\n",
    "        total_steps = dec_inputs.size(1) // self.reduction\n",
    "\n",
    "        attn_hidden = torch.zeros(batch_size, self.hidden_dim, device = self.device)\n",
    "        dec_hidden1 = torch.zeros(batch_size, self.hidden_dim, device = self.device)\n",
    "        dec_hidden2 = torch.zeros(batch_size, self.hidden_dim, device = self.device)\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(total_steps):\n",
    "            if i == 0:\n",
    "                prenet_inputs = torch.zeros(batch_size, self.hidden_dim, device = self.device)\n",
    "            elif self.training:\n",
    "                prenet_inputs = dec_inputs[:,self.reduction * i,:]\n",
    "            else:\n",
    "                prenet_inputs = mel_outputs\n",
    "            \n",
    "            #prenet\n",
    "            prenet_outputs = self.pre_net(prenet_inputs)\n",
    "            #Attention RNN\n",
    "            attn_outputs, attn_hidden = self.attn_rnn(prenet_outputs, attn_hidden)\n",
    "            #Attention\n",
    "            context = self.attention(enc_outputs, attn_outputs)\n",
    "            #Decoder RNN\n",
    "            dec_inputs = torch.cat([attn_outputs, context], dim = -1)\n",
    "            dec_outputs, dec_hidden1, dec_hidden2 = self.dec_rnn(dec_inputs, dec_hidden1, dec_hidden2)\n",
    "            #FC Layer\n",
    "            dec_outputs = self.fc(dec_outputs).view(batch_size, self.hidden_dim, -1)\n",
    "            #Get the next input\n",
    "            mel_outputs = dec_outputs[:,:,-1]\n",
    "            #Gather decoder outputs for each frames\n",
    "            outputs.append(dec_outputs.transpose(1,2))\n",
    "        \n",
    "        return torch.stack(outputs, dim= 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(nn.Module):\n",
    "    def __init__(self, bank_size, proj_size):\n",
    "        super().__init__()\n",
    "        self.cbhg = CBHG(hp.hidden_dim, bank_size, proj_size)\n",
    "    def forward(self, mel_inputs):\n",
    "        return self.cbhg(mel_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import GriffinLim\n",
    "\n",
    "class Vocoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.griffinlim = GriffinLim(hp.n_fft,\n",
    "                                    hp.n_iter, \n",
    "                                    hp.win_length, \n",
    "                                    hp.hop_length, \n",
    "                                    window_fn=torch.hann_window)\n",
    "    \n",
    "    def forward(self, lin_inputs):\n",
    "        return self.griffinlim(lin_inputs)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(hp.prenet_size, hp.enc_bank_size, hp.enc_proj_size)\n",
    "        self.decoder = Decoder(hp.prenet_size)\n",
    "        self.postprocess = PostProcess(hp.ppn_bank_size, hp.ppn_proj_size)\n",
    "        self.vocoder = Vocoder()\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs = self.encoder(enc_inputs)\n",
    "        mel_outputs = self.decoder(enc_outputs, dec_inputs, hp.reduction)\n",
    "        lin_outputs = self.postprocess(mel_outputs) \n",
    "        voc_outputs = self.vocoder(lin_outputs)\n",
    "                \n",
    "        return mel_outputs, lin_outputs, voc_outputs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "830fc09f41f4a8127b783ad2119e0f8b44080ccbdb0580da15d6f44fb7345381"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
